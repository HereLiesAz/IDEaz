# Cortex IDE: Testing Strategy

This document outlines the testing strategy for the Cortex IDE project. A robust testing pyramid will be implemented to ensure the application is reliable, maintainable, and performs as expected. Testing will be a core part of the development workflow for both the Android client and the backend service.

## Guiding Principles
- **Test Pyramid:** We will focus on a healthy test pyramid, with a large base of fast, reliable unit tests, a smaller layer of integration tests, and a very selective set of end-to-end (E2E) tests.
- **Automation:** All tests should be automated and integrated into the CI/CD pipeline. A pull request that breaks existing tests should be blocked from merging.
- **Code Coverage:** While not a perfect metric, we will aim for a high level of code coverage to ensure that all critical paths are tested.

---

## Android Client Testing

The Android client will be tested at three main levels.

### 1. Unit Tests
- **Scope:** Test individual classes in isolation, such as ViewModels, Repositories, and utility functions.
- **Frameworks:** `JUnit 5`, `Mockito-Kotlin` or `MockK` for creating mock objects.
- **Location:** `app/src/test/java`
- **Examples:**
    - Given a specific user action, does the ViewModel update its `StateFlow` to the correct state?
    - Does a Repository correctly parse a successful response from a mocked API?
    - Does a utility function for formatting code work as expected for various inputs?

### 2. Integration Tests
- **Scope:** Test the interaction between different components of the app. These will run on the Android runtime (device or emulator).
- **Frameworks:** `AndroidX Test`, `Espresso` (for traditional views if any), `Room`'s testing library.
- **Location:** `app/src/androidTest/java`
- **Examples:**
    - Does the Room database correctly insert and retrieve data?
    - When a user action is simulated, does the ViewModel correctly fetch data from the Repository and update the state?
    - Does the client-side agent correctly package context from different sources?

### 3. UI / End-to-End (E2E) Tests
- **Scope:** Test the application's UI and user flows from the user's perspective. These are the most expensive tests and will be reserved for critical user journeys.
- **Frameworks:** `Jetpack Compose Testing APIs` (`createComposeRule`).
- **Location:** `app/src/androidTest/java`
- **Examples:**
    - The main task flow: Can a user open a project, tap a UI element, enter a prompt, and see the code and preview update correctly? (This will use a mocked backend).
    - Can a user successfully clone a Git repository?
    - Does the File Explorer UI correctly reflect the file system?

---

## Backend Service Testing

The FastAPI backend will also follow a similar testing pyramid.

### 1. Unit Tests
- **Scope:** Test individual functions and business logic in isolation.
- **Framework:** `pytest`
- **Location:** `tests/unit`
- **Examples:**
    - Does the code chunking utility correctly split a source code file into functions/classes?
    - Does a data validation Pydantic model reject malformed input?

### 2. API Integration Tests
- **Scope:** Test the API endpoints without making external calls to a real LLM or vector database.
- **Framework:** `pytest` with `httpx` and `FastAPI`'s `TestClient`.
- **Location:** `tests/integration`
- **Examples:**
    - Does the `/v1/agent/execute` endpoint return a `200 OK` status for a valid request and a `400 Bad Request` for an invalid one?
    - When the LLM service is mocked, does the API endpoint correctly process the mocked response and return it to the client?
    - Test the logic of the RAG pipeline by mocking the vector DB and the LLM to ensure they are called with the expected data.

### 3. End-to-End (E2E) Tests
- **Scope:** Test the entire backend workflow for a small, controlled scenario. These tests will be run sparingly.
- **Location:** `tests/e2e`
- **Examples:**
    - A full RAG pipeline test:
        1.  Index a small, known set of code documents.
        2.  Send a query to the `/v1/agent/execute` endpoint.
        3.  Assert that the final response generated by the real LLM is reasonable and based on the indexed documents.
    - This will require a dedicated test environment with a running vector DB and access to an LLM.
